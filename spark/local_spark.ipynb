{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import StructField, StructType, StringType, FloatType, IntegerType\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# %pip install spark-nlp\n",
    "from sparknlp.annotator import LemmatizerModel, Tokenizer, Normalizer, StopWordsCleaner, NGramGenerator\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_path = \"/home/ztmj96/.google/credentials/de-r-stocks.json\"\n",
    "\n",
    "# Spark configuration\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"gcs-connector-hadoop3-2.2.5.jar, spark-bigquery-latest_2.12.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_path) \\\n",
    "    .set('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.3')\n",
    "\n",
    "# Spark context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_path)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "\n",
    "# Start Spark session\n",
    "# In production setting, master will be specified during spark-submit\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data from GCS\n",
    "df = spark.read.parquet('gs://datalake_de-r-stocks/stocks/submission/stocks_submission_2022-02-06.parquet')\n",
    "\n",
    "# 1. Remove posts by AutoModerator\n",
    "# 2. Remove duplicate titles\n",
    "# 3. Convert unix timestamp to date\n",
    "# 4. Keep title and date columns\n",
    "df_filter = df.filter(~F.col('author').contains('AutoModerator')) \\\n",
    "    .dropDuplicates(['title']) \\\n",
    "        .withColumn('date', F.from_unixtime(F.col('created_utc'), 'yyyy-MM-dd')) \\\n",
    "            .select('title', 'date')\n",
    "            \n",
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('title') \\\n",
    "     .setOutputCol('title_document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['title_document']) \\\n",
    "     .setOutputCol('title_token')\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['title_token']) \\\n",
    "     .setOutputCol('title_normalized') \\\n",
    "     .setLowercase(True)\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "            .setInputCols(['title_normalized']) \\\n",
    "            .setOutputCol('title_lemma')\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['title_lemma']) \\\n",
    "     .setOutputCol('title_cleaned') \\\n",
    "     .setCaseSensitive(False)\n",
    "\n",
    "ngrams_cum = NGramGenerator() \\\n",
    "            .setInputCols([\"title_cleaned\"]) \\\n",
    "            .setOutputCol(\"title_ngrams\") \\\n",
    "            .setN(2) \\\n",
    "            .setEnableCumulative(True)\\\n",
    "            .setDelimiter(\"_\") # Default is space\n",
    "\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['title_ngrams']) \\\n",
    "     .setOutputCols(['title_finished']) \\\n",
    "     .setCleanAnnotations(False)\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    "              documentAssembler, \n",
    "              tokenizer,\n",
    "              normalizer,\n",
    "              lemmatizer,\n",
    "              stopwords_cleaner,\n",
    "              ngrams_cum,\n",
    "              finisher\n",
    " ])\n",
    "\n",
    "df_result = nlpPipeline.fit(df_filter).transform(df_filter).select('title_finished', 'date')\n",
    "\n",
    "# CountVectorizer model\n",
    "cv = CountVectorizer(inputCol='title_finished', outputCol='features', minDF=3.0)\n",
    "\n",
    "# Train on all submissions\n",
    "model = cv.fit(df_result)\n",
    "\n",
    "df_tokensbydate = df_result.groupBy('date').agg(F.flatten(F.collect_list('title_finished')).alias('title_finished'))\n",
    "\n",
    "# Get counts for each date\n",
    "counts = model.transform(df_tokensbydate).select('date','features').collect()\n",
    "\n",
    "# Create empty dataframe\n",
    "df_wordcountbydate = spark.createDataFrame(spark.sparkContext.emptyRDD(), \n",
    "                        schema=StructType(fields=[\n",
    "                            StructField(\"word\", StringType()), \n",
    "                            StructField(\"count\", FloatType()),\n",
    "                            StructField(\"date\", StringType())]))\n",
    "\n",
    "# Append count for each day to dataframe\n",
    "for row in range(len(counts)):\n",
    "    test_dict = dict(zip(model.vocabulary, (float(x) for x in counts[row]['features'].values)))\n",
    "    df_temp = spark.createDataFrame(test_dict.items(), \n",
    "                        schema=StructType(fields=[\n",
    "                            StructField(\"word\", StringType()), \n",
    "                            StructField(\"count\", FloatType())]))\n",
    "    df_temp = df_temp.withColumn('date', F.lit(counts[row]['date']))\n",
    "    df_wordcountbydate = df_wordcountbydate.unionAll(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wordcountbydate = df_wordcountbydate.withColumn('count', F.col('count').cast(IntegerType())) \\\n",
    "                        .withColumn('submission_date', F.to_date(F.col('date'), 'yyyy-MM-dd')) \\\n",
    "                        .withColumnRenamed('count', 'wordcount') \\\n",
    "                        .drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# upload dataframe to BigQuery\n",
    "df_wordcountbydate.write.format('bigquery') \\\n",
    "  .option('temporaryGcsBucket', 'datalake_de-r-stocks') \\\n",
    "  .save('de-r-stocks.stocks_data.submission_wordcount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the script on the cluster\n",
    "\n",
    "```sh\n",
    "spark-submit \\\n",
    "    --master=\"${URL}\" \\\n",
    "    06_spark_sql.py \\\n",
    "        --input_green=data/pq/green/2021/*/ \\\n",
    "        --input_yellow=data/pq/yellow/2021/*/ \\\n",
    "        --output=data/report-2021\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "gcloud dataproc jobs submit spark --properties spark.jars.packages=com.google.cloud.spark:spark-bigquery_2.11:0.9.1-beta\n",
    "\n",
    "gcloud dataproc jobs submit pyspark --cluster=${CLUSTER} \\\n",
    "    /path/to/your/script.py \\\n",
    "    --jars=gs://hadoop-lib/bigquery/bigquery-connector-hadoop2-latest.jar\n",
    "\n",
    "spark-submit --jars=gs://hadoop-lib/bigquery/bigquery-connector-hadoop2-latest.jar \\\n",
    "    /path/to/your/script.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
